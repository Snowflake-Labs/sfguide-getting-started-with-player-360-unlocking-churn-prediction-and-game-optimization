{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "59a6fe7f-7712-470e-9555-97b0129a9656",
   "metadata": {
    "language": "python",
    "name": "download_packages",
    "collapsed": false,
    "resultHeight": 1609
   },
   "outputs": [],
   "source": "!pip install seaborn\n!pip install scikit-learn\n!pip install matplotlib\n!pip install shap\n!pip install plotly",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "import_packages",
    "resultHeight": 0,
    "collapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport snowflake.snowpark.functions as F \nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nimport shap\nimport plotly.graph_objects as go\nfrom datetime import timedelta\nfrom snowflake.ml.registry import Registry\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n\n# Add a query tag to the session.\nsession.query_tag = {\"origin\":\"sf_sit-is\", \n                     \"name\":\"Player_360\", \n                     \"version\":{\"major\":1, \"minor\":0,},\n                     \"attributes\":{\"is_quickstart\":1}}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "85d89672-8a5a-4fae-be7b-e023e71f021d",
   "metadata": {
    "language": "sql",
    "name": "cell2",
    "resultHeight": 112
   },
   "outputs": [],
   "source": "USE ROLE SYSADMIN;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18c18dcf-4682-4e60-9445-b2f59284b59b",
   "metadata": {
    "language": "python",
    "name": "so_wh",
    "resultHeight": 112,
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"\"\"CREATE OR REPLACE WAREHOUSE so_warehouse WITH\n  WAREHOUSE_SIZE = 'LARGE'\n  WAREHOUSE_TYPE = 'SNOWPARK-OPTIMIZED'\n  RESOURCE_CONSTRAINT = 'MEMORY_16X_X86';\n\"\"\").collect()     ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01c0e942-9599-4a03-bf0e-6b8b7e3c823f",
   "metadata": {
    "language": "sql",
    "name": "use_wh",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "USE WAREHOUSE so_warehouse;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9b549574-9353-4356-9a03-e2c2bea8c6ed",
   "metadata": {
    "name": "rolling_pred_model",
    "collapsed": false,
    "resultHeight": 60
   },
   "source": "## Rolling Predictions Churn Model"
  },
  {
   "cell_type": "code",
   "id": "96dd4d70-22d1-4c28-9c8a-cc7883cbc6f1",
   "metadata": {
    "language": "python",
    "name": "sessions_df_pandas",
    "resultHeight": 439,
    "collapsed": false
   },
   "outputs": [],
   "source": "# gather session information, point per session information, and purchase information\nsession_points_df = session.sql(\"\"\"\nSELECT \ns.session_id,\ns.user_id,\ns.log_in,\ns.session_duration_minutes,\ns.device_type,\nppe.total_points AS total_points_per_session\nFROM DANIEL_PLAYER360_PROD.RAW.SESSIONS s \nLEFT JOIN DANIEL_PLAYER360_PROD.ANALYTIC.POINTS_PER_EVENT ppe ON s.session_id = ppe.session_id\n\"\"\").to_pandas()\nsession_points_df[:100]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1873daea-f455-4f76-9a3d-5491e00137f5",
   "metadata": {
    "language": "python",
    "name": "sessions_eda",
    "resultHeight": 87
   },
   "outputs": [],
   "source": "session_points_df['DEVICE_TYPE'].value_counts()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34784b8d-a016-4171-b2dc-dd5e11f501e7",
   "metadata": {
    "language": "python",
    "name": "sessions_by_days",
    "collapsed": false,
    "resultHeight": 58
   },
   "outputs": [],
   "source": "session_points_df[\"DAY\"] = pd.to_datetime(session_points_df[\"LOG_IN\"].dt.date)\n# Sort the dataframe by USER_ID and LOG_IN to ensure the rolling window works properly\nsession_points_df = session_points_df.sort_values(by=['USER_ID', 'DAY', 'LOG_IN'])\ndf = session_points_df\nlen(df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4efee1d-67a9-40d2-94ab-2afab9927ce3",
   "metadata": {
    "language": "python",
    "name": "window_size",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "# set the window size\nwindow = 30",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29a8ad83-c951-4744-836b-df4c3a3fbf4f",
   "metadata": {
    "language": "python",
    "name": "group_sessions_by_days",
    "resultHeight": 58,
    "collapsed": false
   },
   "outputs": [],
   "source": "day_sessions_df = df.groupby(['USER_ID','DAY']).agg(\n    total_session_duration=('SESSION_DURATION_MINUTES', 'sum'),\n    total_sessions=('SESSION_ID', 'count'),\n    total_points=('TOTAL_POINTS_PER_SESSION', 'sum')\n).reset_index()\n\n# for each user add in the days they were active as 0 and inactive as 1\nday_sessions_df['SESSION_INACTIVE'] = 0\n\nday_sessions_df.columns = [u.upper() for u in list(day_sessions_df.columns)]\nlen(day_sessions_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f9ff359-15de-4876-9611-7c6d4a1c2696",
   "metadata": {
    "language": "python",
    "name": "first_last_login_by_user",
    "resultHeight": 58,
    "collapsed": false
   },
   "outputs": [],
   "source": "# extract the first and last login days for each user to fill the date range \n# allows for faster computation since we only ahve to go 30 indices back instead of checking if each\n# DATE object is within 30 days\nusers_logins_df = df.groupby('USER_ID').agg(\n    first_login_day=('DAY', 'first'),\n    last_login_day=('DAY', 'last')\n).reset_index()\nusers_logins_df.columns = [u.upper() for u in list(users_logins_df.columns)]\nlen(users_logins_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83f99ba0-9eb1-4627-9c61-c88103f3f76a",
   "metadata": {
    "language": "python",
    "name": "fill_date_range",
    "resultHeight": 58,
    "collapsed": false
   },
   "outputs": [],
   "source": "# holds per user, the date range filled sessions information\nfull_day_sessions = []\n\n# for each user, fill the date range for that user and merge back into day_sessions_df\nfor _, row in users_logins_df.iterrows():\n    user_id = row['USER_ID']\n    start_date = row['FIRST_LOGIN_DAY']\n    end_date = row['LAST_LOGIN_DAY']\n   \n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    user_days_df = pd.DataFrame({'USER_ID': user_id, 'DAY': date_range})\n    user_day_sessions = pd.merge(user_days_df, day_sessions_df[day_sessions_df['USER_ID'] == user_id], \n                                 on=['USER_ID', 'DAY'], how='left')\n\n    user_day_sessions['TOTAL_SESSION_DURATION'] = user_day_sessions['TOTAL_SESSION_DURATION'].fillna(0)\n    user_day_sessions['TOTAL_SESSIONS'] = user_day_sessions['TOTAL_SESSIONS'].fillna(0)\n    user_day_sessions['TOTAL_POINTS'] = user_day_sessions['TOTAL_POINTS'].fillna(0)\n    user_day_sessions['SESSION_INACTIVE'] = user_day_sessions['SESSION_INACTIVE'].fillna(1)\n    \n    full_day_sessions.append(user_day_sessions)\n\nday_sessions_df = pd.concat(full_day_sessions).reset_index(drop=True)\nlen(day_sessions_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7552b2e5-620e-4653-a903-d6c214d3b285",
   "metadata": {
    "language": "python",
    "name": "rolling_metrics",
    "resultHeight": 239,
    "collapsed": false
   },
   "outputs": [],
   "source": "day_sessions_df['TOTAL_SESSION_DURATION_ROLLING_30_DAYS'] = day_sessions_df.groupby('USER_ID')['TOTAL_SESSION_DURATION'].rolling(\n    window=window, min_periods=1).sum().reset_index(level=0,drop=True)\nday_sessions_df['TOTAL_SESSIONS_ROLLING_30_DAYS'] = day_sessions_df.groupby('USER_ID')['TOTAL_SESSIONS'].rolling(\n    window=window, min_periods=1).sum().reset_index(level=0,drop=True)\nday_sessions_df['AVERAGE_SESSION_LEN_ROLLING_30_DAYS'] = day_sessions_df['TOTAL_SESSION_DURATION_ROLLING_30_DAYS'] / day_sessions_df['TOTAL_SESSIONS_ROLLING_30_DAYS']\nday_sessions_df['TOTAL_POINTS_ROLLING_30_DAYS'] = day_sessions_df.groupby('USER_ID')['TOTAL_POINTS'].rolling(\n    window=window, min_periods=1).sum().reset_index(level=0,drop=True)\nday_sessions_df['AVERAGE_POINTS_PER_SESSION_ROLLING_30_DAYS'] = day_sessions_df['TOTAL_POINTS_ROLLING_30_DAYS'] / day_sessions_df['TOTAL_SESSIONS_ROLLING_30_DAYS']\nlen(day_sessions_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4b15680-ba30-4a8a-aaa4-7b37e00a90b5",
   "metadata": {
    "language": "python",
    "name": "drop_intermediates_sessions",
    "resultHeight": 57,
    "collapsed": false
   },
   "outputs": [],
   "source": "rolling_sessions_df = day_sessions_df[['USER_ID', 'DAY', 'SESSION_INACTIVE', \\\n                                       'TOTAL_SESSION_DURATION_ROLLING_30_DAYS', \\\n                                      'TOTAL_SESSIONS_ROLLING_30_DAYS', \\\n                                      'AVERAGE_SESSION_LEN_ROLLING_30_DAYS', \\\n                                      'TOTAL_POINTS_ROLLING_30_DAYS', \\\n                                      'AVERAGE_POINTS_PER_SESSION_ROLLING_30_DAYS']]\nlen(rolling_sessions_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5a05cbe-820b-486d-86b8-f42962fcdc68",
   "metadata": {
    "language": "python",
    "name": "remove_30_days",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "def remove_first_30_days(df):\n    df = df.sort_values(by=['USER_ID', 'DAY'])\n    df = df.groupby('USER_ID').apply(lambda x: x.iloc[30:]).reset_index(drop=True)\n    return df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "143afca8-4b28-4c3e-aca9-8cc904943949",
   "metadata": {
    "language": "python",
    "name": "remove_30_days_len",
    "resultHeight": 589
   },
   "outputs": [],
   "source": "# remove the first 29 days metrics for each user_id because we want full 30 day averages\nrolling_sessions_df = remove_first_30_days(rolling_sessions_df)\nlen(rolling_sessions_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3539a81e-b264-472e-a03f-e0a5b0e21d91",
   "metadata": {
    "language": "python",
    "name": "user_1001_rolling",
    "resultHeight": 887
   },
   "outputs": [],
   "source": "user_1001_session_information = rolling_sessions_df[rolling_sessions_df['USER_ID'] == 1001]\nplt.figure(figsize=(10, 6))\nplt.plot(user_1001_session_information['DAY'], \n         user_1001_session_information['TOTAL_SESSION_DURATION_ROLLING_30_DAYS'], label='Total Session Duration (30 days)', color='b', linestyle='-', marker='o')\nplt.plot(user_1001_session_information['DAY'], \n         user_1001_session_information['TOTAL_SESSIONS_ROLLING_30_DAYS'], label='Total Sessions (30 days)', color='g', linestyle='-', marker='x')\nplt.plot(user_1001_session_information['DAY'], \n         user_1001_session_information['AVERAGE_SESSION_LEN_ROLLING_30_DAYS'], label='Average Session Length (30 days)', color='r', linestyle='-', marker='s')\nplt.plot(user_1001_session_information['DAY'], \n         user_1001_session_information['TOTAL_POINTS_ROLLING_30_DAYS'], label='Total Points (30 days)', color='c', linestyle='-', marker='d')\nplt.plot(user_1001_session_information['DAY'], \n         user_1001_session_information['AVERAGE_POINTS_PER_SESSION_ROLLING_30_DAYS'], label='Average Points per Session (30 days)', color='m', linestyle='-', marker='^')\n\ninactive_mask = user_1001_session_information['SESSION_INACTIVE'] == 1\nstart_day = None\nend_day = None\n\n# Plot shaded regions for inactivity periods\nfor i in range(1, len(user_1001_session_information)):\n    if inactive_mask[i] and not inactive_mask[i-1]:\n        # Start of inactivity\n        start_day = user_1001_session_information['DAY'].iloc[i]\n    elif not inactive_mask[i] and inactive_mask[i-1]:\n        # End of inactivity\n        end_day = user_1001_session_information['DAY'].iloc[i-1]\n        # Ensure both start_day and end_day are defined before plotting\n        if start_day is not None and end_day is not None:\n            plt.axvspan(start_day, end_day, color='gray', alpha=0.3, label='Inactive Period' if i == 1 else \"\")\n        start_day = None  # Reset start_day after plotting\n\nplt.xlabel('Day')\nplt.ylabel('Value')\nplt.yscale('log')\nplt.title('Rolling Metrics for USER_ID 1001 (30 days)')\nplt.xticks(rotation=45)\nplt.legend()\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6161167d-d075-46fc-94e8-294da8abefc3",
   "metadata": {
    "language": "python",
    "name": "get_purchases_df",
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# full dataset of all ads\npurchases_df = session.table(\"DANIEL_PLAYER360_PROD.RAW.PURCHASES\").to_pandas()\npurchases_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e40c930d-1640-4a5c-80bb-59957aa14a68",
   "metadata": {
    "language": "python",
    "name": "isolate_purchased_df",
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# dataset only of ads that lead to purchases\npurchased_df = purchases_df[purchases_df['PURCHASE_TYPE'] != 'none']\npurchased_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f5193fe-4ce8-4dde-8316-9ac968f0fb5a",
   "metadata": {
    "language": "python",
    "name": "group_purchases_by_day",
    "resultHeight": 518
   },
   "outputs": [],
   "source": "# get aggregate metrics by day as intermediary to calculate 30 day rolling metrics\npurchases_df['DAY'] = pd.to_datetime(purchases_df['TIMESTAMP_OF_PURCHASE'].dt.date)\nday_purchases_df = purchases_df.groupby(['USER_ID', 'DAY']).agg(\n    total_ad_engagement_time=('AD_ENGAGEMENT_TIME', 'sum'),\n    total_ad_conversions=('AD_CONVERSION', 'sum'),\n    total_ads=('AD_INTERACTION_ID', 'count')\n).reset_index()\nday_purchases_df['PURCHASE_INACTIVE'] = 0\n# merge in day_sessions_df to ensure consistency with date_ranges\nday_purchases_df = pd.merge(day_sessions_df, day_purchases_df, how=\"left\")[list(day_purchases_df.columns)]\nday_purchases_df[list(day_purchases_df.columns)[:-1]] = day_purchases_df[list(day_purchases_df.columns)[:-1]].fillna(0)\nday_purchases_df['PURCHASE_INACTIVE'] = day_purchases_df['PURCHASE_INACTIVE'].fillna(1)\nday_purchases_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23538489-3388-493b-878b-b70318363fd3",
   "metadata": {
    "language": "python",
    "name": "group_purchased_by_day",
    "resultHeight": 428
   },
   "outputs": [],
   "source": "# get aggregate metrics by day as intermediary to calculate 30 day rolling metrics for only purchases\npurchased_df['DAY'] = pd.to_datetime(purchased_df['TIMESTAMP_OF_PURCHASE'].dt.date)\nday_purchased_df = purchased_df.groupby(['USER_ID', 'DAY']).agg(\n    total_purchase_amount=('PURCHASE_AMOUNT', 'sum'),\n    average_purchase_amount=('PURCHASE_AMOUNT', 'mean'),\n    total_purchases = ('PURCHASE_ID', 'count')\n).reset_index()\nday_purchased_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8548dc11-7fd6-4873-bc08-3bb9b78d1319",
   "metadata": {
    "language": "python",
    "name": "merge_results",
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# now perform final ad and purchase merge\nresult_df = pd.merge(day_purchases_df, day_purchased_df, on=['USER_ID', 'DAY'], how='left').fillna(0)\nresult_df.columns = [u.upper() for u in result_df.columns]\nresult_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2fa7761-645c-4200-b37e-8975f111a5da",
   "metadata": {
    "language": "python",
    "name": "result_rolling_30_days",
    "resultHeight": 439
   },
   "outputs": [],
   "source": "result_df['TOTAL_PURCHASE_AMOUNT_ROLLING_30_DAYS'] = result_df.groupby('USER_ID')['TOTAL_PURCHASE_AMOUNT'].rolling(window, min_periods=1).sum().reset_index(level=0, drop=True)\nresult_df['TOTAL_PURCHASES_ROLLING_30_DAYS'] = result_df.groupby('USER_ID')['TOTAL_PURCHASES'].rolling(window, min_periods=1).sum().reset_index(level=0, drop=True)\nresult_df['AVG_PURCHASE_AMOUNT_ROLLING_30_DAYS'] = result_df['TOTAL_PURCHASE_AMOUNT_ROLLING_30_DAYS'] / result_df['TOTAL_PURCHASES_ROLLING_30_DAYS']\nresult_df['TOTAL_ADS_ROLLING_30_DAYS'] = result_df.groupby('USER_ID')['TOTAL_ADS'].rolling(window, min_periods=1).sum().reset_index(level=0, drop=True)\nresult_df['AD_CONVERSION_RATE_ROLLING_30_DAYS'] = result_df['TOTAL_PURCHASES_ROLLING_30_DAYS'] /result_df['TOTAL_ADS_ROLLING_30_DAYS']\nresult_df['TOTAL_AD_ENGAGEMENT_TIME_ROLLING_30_DAYS'] = result_df.groupby('USER_ID')['TOTAL_AD_ENGAGEMENT_TIME'].rolling(window, min_periods=1).sum().reset_index(level=0, drop=True)\nresult_df['AVERAGE_ENGAGEMENT_TIME_ROLLING_30_DAYS'] = result_df['TOTAL_AD_ENGAGEMENT_TIME_ROLLING_30_DAYS'] /result_df['TOTAL_ADS_ROLLING_30_DAYS']\nresult_df['AVG_PURCHASE_AMOUNT_ROLLING_30_DAYS'] = result_df['AVG_PURCHASE_AMOUNT_ROLLING_30_DAYS'].fillna(0)\nresult_df['AD_CONVERSION_RATE_ROLLING_30_DAYS'] = result_df['AD_CONVERSION_RATE_ROLLING_30_DAYS'].fillna(0)\nresult_df['AVERAGE_ENGAGEMENT_TIME_ROLLING_30_DAYS'] = result_df['AVERAGE_ENGAGEMENT_TIME_ROLLING_30_DAYS'].fillna(0)\nresult_df.head(100)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6481ca8c-bd4c-461a-b21a-fb3f6a930257",
   "metadata": {
    "language": "python",
    "name": "drop_intermediates",
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# drop the per day metrics and keep only rolling 30 day metrics\nrolling_purchases_df = result_df[['USER_ID', 'DAY', 'PURCHASE_INACTIVE', \\\n                                  'TOTAL_PURCHASE_AMOUNT_ROLLING_30_DAYS', \\\n                                 'TOTAL_PURCHASES_ROLLING_30_DAYS', \\\n                                 'AVG_PURCHASE_AMOUNT_ROLLING_30_DAYS', \\\n                                 'TOTAL_ADS_ROLLING_30_DAYS', \\\n                                 'AD_CONVERSION_RATE_ROLLING_30_DAYS', \\\n                                 'TOTAL_AD_ENGAGEMENT_TIME_ROLLING_30_DAYS', \\\n                                 'AVERAGE_ENGAGEMENT_TIME_ROLLING_30_DAYS']]\nrolling_purchases_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f82a18b-4de3-4834-970a-d607c68201ba",
   "metadata": {
    "language": "python",
    "name": "len_rolling_pruchases_df",
    "resultHeight": 58
   },
   "outputs": [],
   "source": "len(rolling_purchases_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18361527-3745-41cd-9375-85cd7b0bc4ad",
   "metadata": {
    "language": "python",
    "name": "remove_30_days_rolling_purchases",
    "resultHeight": 210
   },
   "outputs": [],
   "source": "rolling_purchases_df = remove_first_30_days(rolling_purchases_df)\nlen(rolling_purchases_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8713fb62-8e82-467d-9fe2-518f1b977e52",
   "metadata": {
    "language": "python",
    "name": "merge_sessions_purchases",
    "resultHeight": 252
   },
   "outputs": [],
   "source": "# merge sessions and purchases information to have final features dataframe for model training\nfeatures_df = pd.merge(rolling_sessions_df, rolling_purchases_df, on=[\"USER_ID\",\"DAY\"], how=\"outer\")\nfeatures_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d5c2e16-eefe-4f0d-ae0d-01d7074c9bfe",
   "metadata": {
    "language": "python",
    "name": "compress_data_size",
    "resultHeight": 259,
    "collapsed": false
   },
   "outputs": [],
   "source": "features_df['USER_ID'] = features_df['USER_ID'].astype('int32')\n\nbinary_list = ['SESSION_INACTIVE', 'PURCHASE_INACTIVE']\nfeatures_df[binary_list] = features_df[binary_list].astype('int8')\n\ninteger_columns = [u for u in features_df.columns if 'TOTAL' in u] + ['USER_ID'] \ninteger_columns = [col for col in integer_columns  if 'TOTAL_POINTS_ROLLING_30_DAYS' != col] \ninteger_columns\nfeatures_df[integer_columns] = features_df[integer_columns].astype('int32')\n\nfloat_columns = set(features_df.columns) - set(integer_columns) - set(binary_list) - set(['DAY'])\nfeatures_df[list(float_columns)] = features_df[list(float_columns)].astype('float32')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8f405b6-8865-4bdb-b4f4-b3fc659c5cf7",
   "metadata": {
    "language": "python",
    "name": "calculate_label",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Function to compute whether a user logged in in the next 7 days, used for churn labeling\ndef calculate_login_within_7_days(user_data):\n    # Sum the 'SESSION_INACTIVE' values for the next 7 days (excluding current day)\n    # A sum less than 7 means there was at least one active session (SESSION_INACTIVE == 0)\n    user_data['future_sessions_sum'] = user_data['SESSION_INACTIVE'].shift(-1).rolling(window=7, min_periods=1).sum()\n\n    # If the sum is less than 7, then the user logged in within the next 7 days\n    user_data['LOGIN_NEXT_7_DAYS'] = (user_data['future_sessions_sum'] < 7).astype(int)\n\n    # Drop the temporary 'future_sessions_sum' column, if not needed\n    user_data.drop(columns=['future_sessions_sum'], inplace=True)\n\n    return user_data\n\n# Sort the data by USER_ID and DAY\nfeatures_df = features_df.sort_values(by=['USER_ID', 'DAY'])\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2aca948-fdc6-4a3e-b820-6f8f9fd25282",
   "metadata": {
    "language": "python",
    "name": "create_labels",
    "collapsed": false,
    "resultHeight": 591
   },
   "outputs": [],
   "source": "features_df['LOGIN_NEXT_7_DAYS'] = 0\nfeatures_df['LOGIN_NEXT_7_DAYS'] = features_df['LOGIN_NEXT_7_DAYS'].astype(int)\nfeatures_df = features_df.groupby('USER_ID', group_keys=False).apply(calculate_login_within_7_days)\nfeatures_df.head(100)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a727309b-c19d-4f78-8600-3258fe90060a",
   "metadata": {
    "language": "python",
    "name": "predicted_df",
    "collapsed": false,
    "resultHeight": 404
   },
   "outputs": [],
   "source": "# remove the currently active users from the dataset as the users to predict\nretention_df = session.table(\"DANIEL_PLAYER360_PROD.ANALYTIC.RETENTION\").to_pandas()\nactive_users = retention_df[retention_df['CHURNED'] == 0]\ndf1_filtered = features_df[features_df['USER_ID'].isin(active_users['USER_ID'])]\ndf1_filtered = df1_filtered.sort_values(by=['USER_ID', 'DAY'])\nto_pred_df = df1_filtered.groupby('USER_ID').apply(lambda x: x.iloc[-1]).reset_index(drop=True)\n\n# this is the dataset to predicted with our final trained and tested model\nto_pred_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db6d42fe-1910-4fd4-a9fa-b6d98b2878c7",
   "metadata": {
    "language": "python",
    "name": "removed_to_pred_df",
    "collapsed": false,
    "resultHeight": 58
   },
   "outputs": [],
   "source": "# remove from features this dataset of currently active users\nmask = features_df[['USER_ID', 'DAY']].isin(to_pred_df[['USER_ID', 'DAY']]).all(axis=1)\nfeatures_df = features_df[~mask]\nlen(features_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a14384e0-92e5-465c-be6f-efd7eda2fc12",
   "metadata": {
    "language": "python",
    "name": "finalized_features_df",
    "collapsed": false,
    "resultHeight": 252
   },
   "outputs": [],
   "source": "final_features_df = features_df.drop(labels=['USER_ID', 'DAY', 'SESSION_INACTIVE', 'PURCHASE_INACTIVE'], axis=1)\nfinal_features_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3a70820c-3b1d-4679-bb16-c456cc098020",
   "metadata": {
    "name": "eda",
    "collapsed": false,
    "resultHeight": 74
   },
   "source": "# EDA"
  },
  {
   "cell_type": "code",
   "id": "cce3c588-9ce4-4465-91ff-152aacd9d627",
   "metadata": {
    "language": "python",
    "name": "final_features_eda",
    "resultHeight": 357,
    "collapsed": false
   },
   "outputs": [],
   "source": "final_features_df.describe()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2987e3dd-fdd1-4968-ad63-de06e7580713",
   "metadata": {
    "language": "python",
    "name": "heatmap",
    "resultHeight": 891
   },
   "outputs": [],
   "source": "sns.heatmap(final_features_df[list(final_features_df.describe())].corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title(\"Correlation Matrix\")\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a666342-82da-41f4-8ddd-62751b89ab8f",
   "metadata": {
    "language": "python",
    "name": "churned_non_churned",
    "resultHeight": 60,
    "collapsed": false
   },
   "outputs": [],
   "source": "y = features_df['LOGIN_NEXT_7_DAYS']\nchurned_data = final_features_df[y == 0]\nnon_churned_data = final_features_df[y == 1]\nzero_class_count = len(churned_data)\none_class_count = len(non_churned_data)\nprint(zero_class_count)\nprint(one_class_count)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "28157c7b-5d90-445c-9493-d29febb2b3a2",
   "metadata": {
    "name": "model_training",
    "collapsed": false,
    "resultHeight": 74
   },
   "source": "# Model Training"
  },
  {
   "cell_type": "code",
   "id": "f7c72ff1-e1ef-4e08-9422-e7939b9fbbc6",
   "metadata": {
    "language": "python",
    "name": "save_final_features",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# save the dataset as ROLLING_CHURN_FEATURES\nfinal_features_df = session.write_pandas(df=features_df.reset_index(), \\\n                     table_name=\"ROLLING_CHURN_FEATURES\", database=\"DANIEL_PLAYER360_PROD\", schema=\"APP\", \\\n                     quote_identifiers=False,\n                     auto_create_table=True,\n                     overwrite=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab0bce3a-1f63-484c-9ec1-731d18bdaae6",
   "metadata": {
    "language": "python",
    "name": "save_to_pred_features",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "to_pred_df = session.write_pandas(df=to_pred_df.reset_index(), \\\n                     table_name=\"TO_BE_PREDICTED_CHURN_FEATURES\", database=\"DANIEL_PLAYER360_PROD\", schema=\"APP\", \\\n                     quote_identifiers=False,\n                     auto_create_table=True,\n                     overwrite=True).to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5dd43470-1bf8-4928-9c1d-cec13a5661cc",
   "metadata": {
    "language": "python",
    "name": "load_sessions_table",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "final_features_df = session.table(\"DANIEL_PLAYER360_PROD.APP.ROLLING_CHURN_FEATURES\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77a3242d-1cea-45b7-a356-8cf4baf95021",
   "metadata": {
    "language": "python",
    "name": "train_test_split",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# split the dataset into test and train\ntraining, testing = final_features_df.random_split(weights=[0.8, 0.2], seed=111)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8dec5b44-55dd-41e2-b09f-1fa72eeef8e4",
   "metadata": {
    "language": "python",
    "name": "model_creation",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "Target = ['LOGIN_NEXT_7_DAYS']\nOutput_label = ['PREDICTED_CHURN']\nFeatures_label = [\n    \"TOTAL_SESSION_DURATION_ROLLING_30_DAYS\",\n    \"TOTAL_SESSIONS_ROLLING_30_DAYS\",\n    \"AVERAGE_SESSION_LEN_ROLLING_30_DAYS\",\n    \"TOTAL_POINTS_ROLLING_30_DAYS\",\n    \"AVERAGE_POINTS_PER_SESSION_ROLLING_30_DAYS\",\n    \"TOTAL_PURCHASE_AMOUNT_ROLLING_30_DAYS\",\n    \"TOTAL_PURCHASES_ROLLING_30_DAYS\",\n    \"AVG_PURCHASE_AMOUNT_ROLLING_30_DAYS\",\n    \"TOTAL_ADS_ROLLING_30_DAYS\",\n    \"AD_CONVERSION_RATE_ROLLING_30_DAYS\",\n    \"TOTAL_AD_ENGAGEMENT_TIME_ROLLING_30_DAYS\",\n    \"AVERAGE_ENGAGEMENT_TIME_ROLLING_30_DAYS\"\n]\n\nmodel = XGBClassifier(\n    input_cols=Features_label,\n    label_cols=Target,\n    output_cols=Output_label,\n    scale_pos_weight= zero_class_count/one_class_count, # replace with zero_class_count/ one_class_count\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a47b4bb8-cd8b-442c-a2d6-5a297118d14a",
   "metadata": {
    "language": "python",
    "name": "model_fit_training",
    "resultHeight": 1793,
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "training_df = training.to_pandas()\nmodel.fit(training_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa633ec6-ab81-40dd-a7d7-f6cfaeb6607e",
   "metadata": {
    "language": "python",
    "name": "predictions",
    "collapsed": true,
    "resultHeight": 339,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "testing_df =testing.to_pandas()\npredictions = model.predict_proba(testing_df)\ntrue_labels = testing_df['LOGIN_NEXT_7_DAYS']\nchurn_likelihood = predictions[['PREDICT_PROBA_0','PREDICT_PROBA_1']]\nchurn_likelihood.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a743ec0-4813-4f30-8746-e8e9f48f7b4a",
   "metadata": {
    "language": "python",
    "name": "churn_metrics",
    "resultHeight": 239,
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# if the anamolous class (0) has probability >= .5, then didn't login in next 7 days\npredicted_churn = [0 if p >= .4 else 1 for p in churn_likelihood['PREDICT_PROBA_0']]\n# Compute confusion matrix\ncm = confusion_matrix(true_labels, predicted_churn)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Generate classification report\nreport = classification_report(true_labels, predicted_churn)\nprint(\"Classification Report:\")\nprint(report)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "09112ffe-90ee-45e1-8a28-ce18cd77b449",
   "metadata": {
    "name": "model_registry",
    "collapsed": false,
    "resultHeight": 74
   },
   "source": "# Log the Model in the Model Registry"
  },
  {
   "cell_type": "code",
   "id": "77624403-6383-4406-86f1-ae88750b66e6",
   "metadata": {
    "language": "sql",
    "name": "switch_to_app_to_save_model"
   },
   "outputs": [],
   "source": "USE SCHEMA APP;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "27ce771b-31fd-4fe9-9c54-35feebb10c15",
   "metadata": {
    "language": "python",
    "name": "log_model",
    "resultHeight": 689,
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\n\nreg = Registry(session=session)\n\nMODEL_NAME = \"Player360_RollingChurn_Classifier\"\nMODEL_VERSION = \"v1\"\n\nX_train = training_df[Features_label][:1000]\n\nmv = reg.log_model(model,\n                   model_name=MODEL_NAME,\n                   version_name=MODEL_VERSION,\n                   options={\n                       \"case_sensitive\": True,\n                       \"enable_explainability\": True\n    },\n                    sample_input_data=X_train\n)\n\nmv.set_metric(\"confusion_matrix\", cm.tolist())\nmv.set_metric(\"classification_report\", report)\n\nmv.comment = \"This is the first iteration of our Rolling Churn Classification model.\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d89155bc-a8a9-4eb4-af89-1650f8f68684",
   "metadata": {
    "language": "python",
    "name": "check_models",
    "resultHeight": 112,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "reg.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e7b4f94-fc5d-44ee-8b9d-8a53976af127",
   "metadata": {
    "language": "python",
    "name": "test_run",
    "resultHeight": 239,
    "codeCollapsed": false,
    "collapsed": true
   },
   "outputs": [],
   "source": "X_test = testing.select(Features_label).limit(10)\nmv.run(X_test, function_name=\"predict_proba\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6771d7d2-6134-402e-8150-9eff898f047f",
   "metadata": {
    "name": "model_explainability",
    "collapsed": false,
    "resultHeight": 74
   },
   "source": "# Model Explainability"
  },
  {
   "cell_type": "code",
   "id": "3e61e17c-ccc8-4d48-882d-005b59230325",
   "metadata": {
    "language": "python",
    "name": "mv_explanations_exp",
    "resultHeight": 239,
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "mv_explanations = mv.run(X_test, function_name=\"explain\")\nmv_explanations =mv_explanations.to_pandas()\nmv_explanations.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "190c55eb-f311-49a8-80ea-0c73195982e2",
   "metadata": {
    "language": "python",
    "name": "shap_plot",
    "resultHeight": 239,
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "shap_exp = shap._explanation.Explanation(mv_explanations.values, feature_names=mv_explanations.columns)\nshap.plots.bar(shap_exp)",
   "execution_count": null
  }
 ]
}